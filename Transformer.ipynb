{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa5e219",
   "metadata": {},
   "source": [
    "# Encoder-Only LLM Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b32d79",
   "metadata": {},
   "source": [
    "### ➨ Python Class (opening notes)\n",
    "\n",
    "Classes python possess several advantages.\n",
    "\n",
    "1. Encapsulation: related functions and data into a single unit.\n",
    "\n",
    "2. Abstraction: abstract away implementation details and expose only the necessary interfaces to the outside world. \n",
    "\n",
    "3. Code Reusability: once you've defined a class, you can create multiple instances of that class (objects) with different data. This allows you to reuse the same code in different parts of your program or even in different programs altogether.\n",
    "\n",
    "4. Inheritance: classes support inheritance, which allows you to create new classes that inherit attributes and methods from existing classes. This promotes code reuse and helps you build on existing functionality without having to reinvent the wheel.\n",
    "\n",
    "The name of the class is, for example \n",
    "```python\n",
    "class class_name(param_1,...,param_n):\n",
    "```\n",
    "where the parameters are the parent classes for inheritance.\n",
    "\n",
    "#### Important PyTorch note:\n",
    "In PyTorch, when defining a class (e.g., a new network model), it inherits from the class `nn.Module`, so we should write\n",
    "```python\n",
    "class net(nn.Module):\n",
    "```\n",
    "The class `nn.Module` always calls for a function named `forward`, so we need to define this function, which represets the forward-pass and subsequently automatically defines the backword-pass for backpropagation. This is why it is enough to call the class, e.g. `model = net(parameters)` (where we specify the required `parameters` when writing the `__init__` method of our new class `net`) for the forward pass, unlike other classes which require calling the functions of the class separately.\n",
    "\n",
    "### Constructor (`__init__`), `self` and `super()`\n",
    "\n",
    "##### Constructor\n",
    "A *constructor method*, also known as a constructor, is a method in OOP languages that is automatically called when an instance of a class is created. It initializes the newly created object. In Python, the constructor is defined with `__init__`.\n",
    "The constructor is often used to initialize instance variables or perform setup when creating an object. It allows to customize how objects are initialized when they are created from a class.\n",
    "\n",
    "##### Self\n",
    "In Python, `self` is a conventionally used name for the first parameter of instance methods in a class. When you create an instance of a class and call a method on that instance, Python automatically passes the instance as the first argument to the method. It's a way for the method to refer to the *specific* instance it's operating on.\n",
    "\n",
    "##### Super\n",
    "In Python, `super()` is a built-in function that is typically used to call methods defined in the superclass (parent class) within a subclass (child class).\n",
    "Using `super()` allows for more maintainable and extensible code by ensuring that changes in inheritance hierarchy propagate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "237f9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f208e",
   "metadata": {},
   "source": [
    "# ➨ The Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddc3fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "# defines a new class named TransformerModel, which inherits from nn.Module. \n",
    "# this indicates that TransformerModel is a PyTorch NN module and can make use of various features provided by PyTorch.\n",
    "# see cells below for iformation about __init__, self, and super()\n",
    "\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        # for dropout we provide a float parameter with default 0.5 if not provided\n",
    "        \n",
    "        super().__init__()  # call the constrcutor of the superclass nn.Module for inheritance\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        # initializes pos_encoder with an instance of the PositionalEncoding class \n",
    "        # (the class is expected to be defined elsewhere in the code)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)  \n",
    "        \n",
    "        # initializes an instance of the TransformerEncoderLayer class (one encoder layer) with the provided parameters\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) \n",
    "        \n",
    "        # initializes an instance of the TransformerEncoder class (one encoder) with the provided one encoder layer\n",
    "        # (see below for more information) \n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)  \n",
    "        \n",
    "        # an embedding layer that maps each token (indexed by integers) to a vector of dimension d_model\n",
    "        self.embedding = nn.Embedding(ntoken, d_model) \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # maps the output of the Transformer model to a vector of size ntoken, \n",
    "        # representing the logits (scores) for each token in the vocabulary\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        # calls init_weights (see function below) to initialize the weights of the embedding layer and the linear layer\n",
    "        self.init_weights()  \n",
    "\n",
    "        \n",
    "    # a method for weight initialization that takes no arguments and returns None\n",
    "    def init_weights(self) -> None:  \n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)  # initializes the weights of the embedding layer\n",
    "        self.linear.bias.data.zero_()  # initializes the bias of the linear layer to zeros\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)  # initializes the weights of the linear layer\n",
    "\n",
    "    \n",
    "    # defines the forward pass of the model\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:      \n",
    "        \"\"\"Arguments:\n",
    "        src: Tensor of shape [seq_len, batch_size] (this is batch of data)\n",
    "        src_mask: Tensor of shape [seq_len, seq_len] (a square matrix representing tokens to mask; \n",
    "                                                      if the upper right triangle is masked, then this is future masking \n",
    "                                                      which is also the default masking of this code)\n",
    "        Returns: output Tensor of shape [seq_len, batch_size, ntoken] (logits for each column in the batch)\"\"\"\n",
    "        \n",
    "        # embeds the input tokens (src) and scales them by the square root of the d_model;\n",
    "        # this scaling factor is used to prevent the gradients from becoming too small or too large during training\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # applies positional encoding to the embedded input tokens\n",
    "        src = self.pos_encoder(src) \n",
    "        \n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0) (causal masking is when we mask the future, \n",
    "                                                           that is when we train the model such that it will choose the next \n",
    "                                                           token conditioned on previous choices; \n",
    "                                                           so this default masks the upper left triangle)\"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        \n",
    "        # passes the embedded and encoded input tokens through the transformer encoder (transformer_encoder) \n",
    "        # to obtain the output representations\n",
    "        output = self.transformer_encoder(src, src_mask) \n",
    "        \n",
    "        # applies a linear transformation to the output representations to obtain the logits for each token in the vocabulary\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f3285",
   "metadata": {},
   "source": [
    "### The TransformerEncoderLayer Class\n",
    "\n",
    "    CLASS torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=function relu, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)\n",
    "\n",
    "\n",
    "TransformerEncoderLayer is made up of self-attention and FF network. This standard encoder layer is based on the paper “Attention Is All You Need”, where:\n",
    "1. d_model (int) – the number of expected features in the input (required).\n",
    "\n",
    "2. nhead (int) – the number of heads in the multiheadattention models (required).\n",
    "\n",
    "3. dim_feedforward (int) – the dimension of the feedforward network model (default=2048).\n",
    "\n",
    "4. dropout (float) – the dropout value (default=0.1).\n",
    "\n",
    "5. activation (Union[str, Callable[[Tensor], Tensor]]) – the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu\n",
    "\n",
    "6. layer_norm_eps (float) – the eps value in layer normalization components (default=1e-5).\n",
    "\n",
    "7. batch_first (bool) – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False (seq, batch, feature).\n",
    "\n",
    "8. norm_first (bool) – if True, layer norm is done prior to attention and feedforward operations, respectively. Otherwise it’s done after. Default: False (after).\n",
    "\n",
    "9. bias (bool) – If set to False, Linear and LayerNorm layers will not learn an additive bias. Default: True.\n",
    "\n",
    "\n",
    "This class has the following function: \n",
    "\n",
    "    forward(src, mask=None, src_key_padding_mask=None, is_causal=None)\n",
    "\n",
    "where:\n",
    "1. src (Tensor) – the sequence to the encoder (required).\n",
    "2. mask (Optional[Tensor]) – the mask for the src sequence (optional).\n",
    "3. src_key_padding_mask (Optional[Tensor]) – the mask for the src keys per batch (optional).\n",
    "4. is_causal (Optional[bool]) – If specified, applies a causal mask as mask. Default: None; try to detect a causal mask. Warning: is_causal provides a hint that mask is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.\n",
    "\n",
    "Example:\n",
    "> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)<br>\n",
    "src = torch.rand(10, 32, 512)<br>\n",
    "ut = encoder_layer(src)\n",
    "\n",
    "Alternatively, when batch_first is True:\n",
    "> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)<br>\n",
    "src = torch.rand(32, 10, 512)<br>\n",
    "ut = encoder_layer(src)\n",
    "\n",
    "### The TransformerEncoder Class works\n",
    "\n",
    "    CLASS torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None, enable_nested_tensor=True, mask_check=True)\n",
    "\n",
    "Where:\n",
    "1. encoder_layer – an instance of the TransformerEncoderLayer() class (required).\n",
    "2. num_layers – the number of sub-encoder-layers in the encoder (required).\n",
    "3. norm – the layer normalization component (optional).\n",
    "4. enable_nested_tensor – if True, input will automatically convert to nested tensor (and convert back on output). This will improve the overall performance of TransformerEncoder when padding rate is high. Default: True (enabled).\n",
    "\n",
    "This class has the same function `forward` as in the TransformerEncoderLayer class.\n",
    "\n",
    "Example:\n",
    "> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)<br>\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)}<br>\n",
    "src = torch.rand(10, 32, 512)<br>\n",
    "out = transformer_encoder(src)\n",
    "\n",
    "### The Embedding Layer\n",
    "For example, if the vocabulary size is 10, then `emb = nn.Embdeding(10, 3)` creates 10 vectors (tensors) of size 3, each represents a different word. These parameters are learnable, and are randomly initialized from the unit Gaussian. In this LLM code, the initialization is done uniformly using a different dedicated function `init_weight`. Other layer features exist.\n",
    "\n",
    "The layer `emb` takes input indices, each index represent a word. For example:\n",
    "\n",
    "    >>> embedding = nn.Embedding(10, 3)\n",
    "    >>> seq = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])  # a batch of 2 sequences with 4 tokens each\n",
    "    >>> embedding(seq)\n",
    "    tensor([[[-0.0251, -1.6902,  0.7172],\n",
    "             [-0.6431,  0.0748,  0.6969],\n",
    "             [ 1.4970,  1.3448, -0.9685],\n",
    "             [-0.3677, -2.7265, -0.1685]],\n",
    "    \n",
    "            [[ 1.4970,  1.3448, -0.9685],\n",
    "             [ 0.4362, -0.4004,  0.9400],\n",
    "             [-0.6431,  0.0748,  0.6969],\n",
    "             [ 0.9124, -2.3616,  1.1151]]])\n",
    "Notice that, for example, the row (embedding) corresponfing to the token indexed 4 appears in the correct place in each sequence (tensor). Of course, each token index can correspond to a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931ad15",
   "metadata": {},
   "source": [
    "# ➨ Positional Encoding\n",
    "\n",
    "The `PositionalEncoding` module injects some information about the relative or absolute position of the tokens in the sequence. \n",
    "The positional encodings have the same dimension as the embeddings so that the two can be summed. \n",
    "Here, we use sine and cosine functions of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd1be93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "# defines a new class named PositionalEncoding, which inherits from nn.Module. \n",
    "# this indicates that TransformerModel is a PyTorch NN module and can make use of various features provided by PyTorch.\n",
    "\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # creates a tensor containing integers from 0 to max_len-1, and unsqueezes it to add a new dimension at index 1\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # registers the pe tensor as a buffer of the module.\n",
    "        # buffers are tensors that are not updated by gradients during training,\n",
    "        # and they are typically used for parameters that are not trainable, such as fixed positional encodings\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Arguments: x Tensor of shape [seq_len, batch_size, embedding_dim]\"\"\"\n",
    "        # adds the positional encodings (pe) to the input tensor x\n",
    "        # he positional encodings are added to the first seq_len elements of the input tensor x, \n",
    "        # where seq_len is the length of the input sequence\n",
    "        x = x + self.pe[:x.size(0)]  \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69a502",
   "metadata": {},
   "source": [
    "# ➨ Vocabulary and Datasets\n",
    "The `vocab` object is built based on the training dataset, and is used to numericalize tokens into tensors. The data set `PennTreebank` represents rare tokens as the `<unk>` token.\n",
    "\n",
    "Given a 1D vector of sequential data, the `batchify()` function arranges the data into `batch_size` columns. If the data does not divide evenly into `batch_size` columns, then the data is trimmed to fit. Batching enables more parallelizable processing, however batching means that the model treats each column independently and dependence of columns cannot be learned.\n",
    "\n",
    "We point out that by 'sequential data' or 'sequential tokens' we mean data of coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "732493bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the torchtext library provides utilities for working with text data\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "############ Generate the vocabulary ############\n",
    "\n",
    "# loads the training split of the PennTreebank dataset using the PennTreebank class from the torchtext.datasets module.\n",
    "# using the training split, we create the vocabulary (as well as the training dataset)\n",
    "train_iter = PennTreebank(split='train')\n",
    "\"\"\"train_iter is a PennTreebank iterable object containing 42068 training sentences.\n",
    "   To read all sentences, use the code: [print(i) for i in train_iter]\"\"\"\n",
    "\n",
    "# generates a tokenizing function that converts string sentences into string tokens using the 'basic_english' tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')  \n",
    "\n",
    "# map(tokenizer, train_iter) applies the function tokenizer elemnt-wise to each sentence in train_iter, \n",
    "# producing an iterable object of tokens, where each sentence is converted into a list of string tokens\n",
    "token_iter = map(tokenizer, train_iter)\n",
    "\"\"\"To read all lists of tokens, use the code: [print(i) for i in map(tokenizer, train_iter)]\"\"\"\n",
    "\n",
    "# build_vocab_from_iterator builds a vocabulary from an iterable object of tokens by adding indices to each token, \n",
    "# and adds a certain token (if not already exists) to be designated as a special token\n",
    "vocab = build_vocab_from_iterator(token_iter, specials=['<unk>'])\n",
    "\"\"\"To read tokens in indices 1,2, use the code: print(vocab.lookup_tokens([1,2]))\"\"\"\n",
    "\n",
    "# set_default_index(vocab['<unk>']) sets the default index for out-of-vocabulary tokens to the index of <unk>\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# NOTICE that the vocabulary object `vocab` is defined using the training set #\n",
    "\n",
    "\n",
    "############ Generate the datasets ############\n",
    "\n",
    "# data_process takes an iterable object of raw text, and converts it into a flat tensor of corresponding token indices:\n",
    "# each item (sentence) in raw_text_iter is tokenized using the tokenizer function,\n",
    "# mapped to token indices using the vocabulary, and then converted to a tensor of type torch.long\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]  # list of tensors\n",
    "    \"\"\"tokenizer(item) is a list (sentence) of string tokens, and vocab(tokenizer(item)) are their indices in the vocabulary\"\"\"\n",
    "    \n",
    "    # filter(lambda t: t.numel() > 0, data) filters all empty tensors which may occur if some tokens are not in the vocabulary.\n",
    "    # tuple the list of (filtered) tensor `data` into (ordered) tuple of tensors  \n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))  # concatenates the tuple of tensors into a single tensor\n",
    "\n",
    "\n",
    "# load the training, validation, and test splits of the PennTreebank dataset, and process them using data_process\n",
    "train_iter, val_iter, test_iter = PennTreebank()  # without the positional embeddings\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "\n",
    "# checks if a CUDA-capable GPU is available and selects the device accordingly (cuda if available, otherwise cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# takes a flat tensor of token indices `data` and a batch size `bsz`, and converts it into a batched tensor for training: \n",
    "# it reshapes the data into `bsz` separate sequences (columns), discarding any extra elements that wouldn't fit,\n",
    "# and the resulting tensor has shape [N//bsz, bsz], where N is the length of the original flat data tensor.\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "# NOTICE that after applying batchify(), each column is a batch of sequential tokens, and the columns allow parallelization #\n",
    "# NOTICE that since the columns are processes in parallel, no relation between column can be learned #\n",
    "\n",
    "\n",
    "# batchify the training, validation, and test data using batchify\n",
    "batch_size, eval_batch_size = (20, 10)  # number of columns of sequential tokens for learning and evaluation\n",
    "train_data = batchify(train_data, batch_size)\n",
    "\"\"\"To read the sequential tokens that form the the first column of the training data, use the code:\n",
    "   print(vocab.lookup_tokens([*train_data[:,0]]))\"\"\"\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9278b",
   "metadata": {},
   "source": [
    "### The `get_tokenizer` Function\n",
    "\n",
    "`torchtext.data.utils.get_tokenizer(tokenizer, language='en')` generates a tokenizer function for a string sentence, where:\n",
    "\n",
    "1. tokenizer: the name of tokenizer function. If None, it returns `split()` function, which splits the string sentence by space. If basic_english, it returns `_basic_english_normalize()` function, which normalize the string first and split by space. If a callable function, it will return the function. If a tokenizer library (e.g. spacy, moses, toktok, revtok, subword), it returns the corresponding library.\n",
    "2. language – Default `en`.\n",
    "\n",
    "For example,\n",
    "\n",
    "    >>> tokenizer = get_tokenizer(\"basic_english\")\n",
    "    >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "    >>> tokens\n",
    "    >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\\\n",
    "    \n",
    "### The `build_vocab_from_iterator` Function\n",
    "\n",
    "    torchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None) → Vocab\n",
    "    \n",
    "Builds a Vocab object from an iterator (refer to the literature of the Vocab class for information), where:\n",
    "\n",
    "1. iterator – Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
    "2. min_freq – The minimum frequency needed to include a token in the vocabulary.\n",
    "3. specials – Special symbols to add. The order of supplied tokens will be preserved.\n",
    "4. special_first – Indicates whether to insert symbols at the beginning or at the end.\n",
    "5. max_tokens – If provided, creates the vocab from the `max_tokens - len(specials)` most frequent tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd21a8e",
   "metadata": {},
   "source": [
    "# ➨ Batching the Data\n",
    "`get_batch()` generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length `bptt` (size of context window). In other words, `get_batch()` generates the `(input, label)` pairs where:\n",
    "1. `input` is a sequence of sequential tokens of length `bptt`.\n",
    "1. `label` is the target sequence of sequential tokens of length `bptt`, which is the same as `input` but with an indent of one index (as in encoder-only auto-regressuve architectures).\n",
    "\n",
    "We also refer to a single pair of `(input, label)` as a mini-batch. The `get_batch()` function works across all batches in parallel (that is, it works column-wise). This function takes a data tensor of size `[N//batch_size, batch_size]`, where N is the length of the original flat data tensor, and an integer `i` specifying the current position of the context window. The function returns a tuple of tensors `(input, label)`, where `input` is of size `[seq_len, batch_size]`, where `seq_len` is at most the size of the context window `bptt` (it can be shorter if the current sentence is shorter), and `label` is of size `[seq_len * batch_size]` (suitable for CE loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d0a2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35  # the maximal seq_len, that is the size of the context window (sequences can be shorter, depending on the source)\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    seq_len = min(bptt, len(source) - 1 - i)  # ensures that seq_len does not exceed bptt or go beyond the end of the source\n",
    "    data = source[i:i+seq_len]  # extracts the input sequence from the source (column-wise)\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)  # extracts the target from the source\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadb28e",
   "metadata": {},
   "source": [
    "# ➨ Defining the Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95d738bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an instance of the network\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)  # an instance of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619b219",
   "metadata": {},
   "source": [
    "Once we have an instance of the network (with feed-forward and back-propagation pass functions established), we build the entire model, which includes the loss, training process and evaluation process.\n",
    "\n",
    "We use CrossEntropyLoss with the SGD optimizer. The learning rate is initially set to 5.0 and follows a StepLR schedule (a step-size that decays by a factor of $\\gamma$ every a speceified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f62ebc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # initial learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)  # lr decay by gamma=0.95 every step_size=1 epochs\n",
    "\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    # turn on training mode (some layers like dropout and batch norma behave differently during training and evaluation)\n",
    "    model.train()\n",
    "    \n",
    "    num_batches = len(train_data) // bptt  # the number of (input,label) pairs in the training data\n",
    "    # NOTICE that by `num_batches` we mean the number of mini-batches, which are pairs of (input,label) sequences, that are\n",
    "    # fed to the model. Recall that each mini-bitch is a pair of tensors of size [seq_len, batch_size]. # \n",
    "    \n",
    "    log_interval = 200  # interval of mini-batches for logging training progress\n",
    "    total_loss = 0.  # average time taken for each `log_intervals' of mini-batches\n",
    "    start_time = time.time()  # average time taken for each `log_intervals' of mini-batches \n",
    "    \n",
    "      \n",
    "    # the enumerate function generates a pair of indices (batch, i), where `batch` starts at 0 and ends at len(train_data)\n",
    "    # with intervals of size `bptt`; hence `batch` corresponds to the row index of the training data tensor at which\n",
    "    # the current mini-batch starts.\n",
    "    # the index `i` is the index of the current mini-batch, hence it starts at 0 and ends at `num_batches`\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):  \n",
    "        data, targets = get_batch(train_data, i)  # (data, target) are (input,label) pair, both of size [seq_len, batch_size]\n",
    "        \n",
    "        output = model(data)  # the output is of size [seq_len, batch_size, vocab_size]\n",
    "        # NOTICE that for each element of the `batch_size` columns (each column is a sequence of tokens), \n",
    "        # we have a vecotr of logits of the size of the vocabulary #\n",
    "        \n",
    "        output_flat = output.view(-1, ntokens)  # reshapes the output to [batch_size * seq_len, ntokens] for CE loss\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # during training, we use nn.utils.clip_grad_norm_ to prevent gradients from exploding; this function\n",
    "        # clips the gradient such that their norm is 0.5 by normalization; the norm is computed over all gradients together, \n",
    "        # as if they were concatenated into a single vector.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        total_loss += loss.item()  # update the total loss of by summing over all last `log_interval` mini-batches\n",
    "        if batch % log_interval == 0 and batch > 0:  # checks if it is time to log training progress\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            \n",
    "            # average time (in ms), loss and perplexity of the last `log_interval` mini-batches of the epoch\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval  \n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            \n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            \n",
    "            # reinitialize loss and time for the next `log_intervals` of mini-batch\n",
    "            total_loss = 0  \n",
    "            start_time = time.time()\n",
    "\n",
    "            \n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval() \n",
    "    total_loss = 0.\n",
    "    \n",
    "    # no_grad() disables gradient calculation; useful for inference when we do not call backward().\n",
    "    # reduces memory consumption for computations that would otherwise have `requires_grad=True`.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):  # same as in training\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)  # seq_len of the current evaluation mini-batch (up to `bptt`)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            \n",
    "            # unlike training, during evaluation we calculate the loss over the entire epoch (all mini-batches);\n",
    "            # therefore, we multiply each loss by the size of the current mini-batch (which is `seq_len`),\n",
    "            # and last we divide the sum by the length of the evaluation/validation data\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()  \n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dac90",
   "metadata": {},
   "source": [
    "# ➨ Training and Model Evaluation\n",
    "During training, we check what are the best learned parameters so far by testing them on the validation set, and keep them as the output model.\n",
    "\n",
    "Notice that the mini-batches for SGD are not taken randomly, by rather in a sequential order of `(input,label)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e457099f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1320 batches | lr 5.00 | ms/batch 110.09 | loss  6.88 | ppl   975.11\n",
      "| epoch   1 |   400/ 1320 batches | lr 5.00 | ms/batch 103.65 | loss  6.06 | ppl   426.62\n",
      "| epoch   1 |   600/ 1320 batches | lr 5.00 | ms/batch 109.26 | loss  5.83 | ppl   340.41\n",
      "| epoch   1 |   800/ 1320 batches | lr 5.00 | ms/batch 116.12 | loss  5.66 | ppl   286.47\n",
      "| epoch   1 |  1000/ 1320 batches | lr 5.00 | ms/batch 129.05 | loss  5.58 | ppl   265.86\n",
      "| epoch   1 |  1200/ 1320 batches | lr 5.00 | ms/batch 131.20 | loss  5.48 | ppl   238.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 162.26s | valid loss  5.46 | valid ppl   234.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1320 batches | lr 4.75 | ms/batch 132.02 | loss  5.41 | ppl   223.23\n",
      "| epoch   2 |   400/ 1320 batches | lr 4.75 | ms/batch 135.60 | loss  5.35 | ppl   211.22\n",
      "| epoch   2 |   600/ 1320 batches | lr 4.75 | ms/batch 138.83 | loss  5.31 | ppl   202.99\n",
      "| epoch   2 |   800/ 1320 batches | lr 4.75 | ms/batch 142.87 | loss  5.25 | ppl   191.04\n",
      "| epoch   2 |  1000/ 1320 batches | lr 4.75 | ms/batch 151.07 | loss  5.26 | ppl   191.93\n",
      "| epoch   2 |  1200/ 1320 batches | lr 4.75 | ms/batch 143.31 | loss  5.17 | ppl   176.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 190.87s | valid loss  5.30 | valid ppl   200.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1320 batches | lr 4.51 | ms/batch 139.03 | loss  5.17 | ppl   176.44\n",
      "| epoch   3 |   400/ 1320 batches | lr 4.51 | ms/batch 141.55 | loss  5.14 | ppl   170.41\n",
      "| epoch   3 |   600/ 1320 batches | lr 4.51 | ms/batch 146.95 | loss  5.11 | ppl   165.86\n",
      "| epoch   3 |   800/ 1320 batches | lr 4.51 | ms/batch 147.74 | loss  5.06 | ppl   158.10\n",
      "| epoch   3 |  1000/ 1320 batches | lr 4.51 | ms/batch 143.01 | loss  5.09 | ppl   162.31\n",
      "| epoch   3 |  1200/ 1320 batches | lr 4.51 | ms/batch 151.97 | loss  5.01 | ppl   149.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 198.38s | valid loss  5.24 | valid ppl   188.20\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')  # for tracking the best validation loss encountered during training\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# create a temporary directory using the TemporaryDirectory; used to store the parameters of the best model during training\n",
    "with TemporaryDirectory() as tempdir:  \n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")  # creates a path to store the parameters\n",
    "\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()  # track the overall time of each epoch (s)\n",
    "        train(model)  # train the model for a single epoch\n",
    "        \n",
    "        # calculate the loss and perplexity over the validation set using the trained model (a single epoch)\n",
    "        val_loss = evaluate(model, val_data)  \n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time  # overall time of current epoch\n",
    "        \n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "        \n",
    "        \n",
    "        # checks if the current validation loss is better than the previous best validation loss `best_val_loss`.\n",
    "        # if so, update `best_val_loss` with the current validation loss and save the parameters of the current best model\n",
    "        # to the specified file path.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            \n",
    "        scheduler.step()  # adjusts the learning rate scheduler after each epoch\n",
    "        \n",
    "        \n",
    "    # after training is completed (all epoch are done), load the parameters of the best model for evaluation over test set\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b6c9c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  6.98 | test ppl  1071.73\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# evaluate the best model on the test dataset\n",
    "\n",
    "test_loss = evaluate(model, test_data)  # overall loss over the test set\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
